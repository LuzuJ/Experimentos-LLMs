# üß¨ Symbolic-AutoML: Evoluci√≥n de Optimizadores Zero-State con LLMs Locales

Este repositorio documenta un experimento de Descubrimiento Algor√≠tmico Automatizado (Symbolic Discovery) utilizando un modelo de lenguaje de 4 Billones de par√°metros ejecutado localmente en una CPU de consumidor.

El objetivo: Demostrar que t√©cnicas usadas por Google DeepMind para descubrir algoritmos (como el optimizador Lion) pueden ser replicadas a microescala para resolver problemas de nicho, espec√≠ficamente en Edge Computing (TinyML) y entornos con etiquetas ruidosas.

üß† El Descubrimiento: "Robust Zero-State Optimizer"

A trav√©s de un ciclo evolutivo (Algoritmo Gen√©tico guiado por LLM), el modelo evalu√≥ iterativamente funciones matem√°ticas en un Sandbox seguro. Le impusimos dos restricciones extremas:

Memoria O(1): Prohibido usar tensores del tama√±o de la red para almacenar historial (como hacen Adam o Lion). Solo pod√≠a usar memoria escalar.

Ruido Extremo: El 20% de las etiquetas del dataset de evaluaci√≥n fueron corrompidas (invertidas) artificialmente.

La Matem√°tica Emergente (Generaci√≥n 166)

El LLM redescubri√≥ de forma aut√≥noma el uso de estad√≠stica robusta combinada con la funci√≥n de Signo (SignSGD parcial) para crear una Votaci√≥n Democr√°tica de Capa. La funci√≥n descubierta fue:

## Matem√°tica extra√≠da del LLM

robust_grads = torch.sign(p.grad)
median_grad = torch.median(robust_grads)

## Inercia escalar aislada de la magnitud del ruido

state['m'] = c1 * state['m'] + (1 - c1) * median_grad
scale = 1.0 / (torch.abs(state['m']) + 1.0)

p.sub_(robust_grads, alpha=(lr * scale))

### üìä Resultados Emp√≠ricos (Benchmark vs Adam)

Se evalu√≥ la heur√≠stica extra√≠da frente a Adam en una Red Neuronal Convolucional entrenando el dataset Fashion-MNIST con un 20% de inyecci√≥n de ruido sim√©trico en las etiquetas.

Adam: Inicialmente aprende, pero memoriza r√°pidamente el ruido (overfitting), degradando su capacidad de generalizaci√≥n y estanc√°ndose.

Nuestro Optimizador (IA): Al ignorar la magnitud del gradiente y filtrar anomal√≠as con la mediana de los signos, a√≠sla la se√±al √∫til del ruido, superando el accuracy final de Adam sin utilizar la memoria RAM extra que este √∫ltimo requiere.

![alt text](image/results_vs_Adam.png)

üõ†Ô∏è Arquitectura del Motor Evolutivo

El sistema fue dise√±ado para operar en recursos limitados (Ryzen 7 7730U, 16GB RAM):

Filtro de Alucinaciones: An√°lisis AST (Abstract Syntax Tree) en subprocesos para atrapar c√≥digo inv√°lido en <1ms.

Amnesia Controlada: El LLM (Qwen-4B) solo recibe sus √∫ltimos 3 errores y una "Pizarra de Laboratorio" con principios matem√°ticos fijos para evitar la diluci√≥n del contexto.

Evaluaci√≥n Proxy: Redes estoc√°sticas min√∫sculas para evaluar el fitness en milisegundos.

üöÄ Pr√≥ximos Pasos (Tesis)

El motor evolutivo ser√° reconfigurado para relajar las restricciones de memoria y explorar paradigmas emergentes de optimizaci√≥n a gran escala, compitiendo contra arquitecturas de estado del arte como Lion (EvoLved Sign Momentum) y Sophia (Second-order Clipped Stochastic Optimization).

Este proyecto es parte de la investigaci√≥n para tesis de grado en paradigmas emergentes de optimizaci√≥n neuronal.
